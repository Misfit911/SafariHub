{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images\\Hub_title.png\" alt=\"[YOUR_IMAGE_ALT]\">\n",
    "</p>\n",
    "\n",
    "## ***Business Understanding***\n",
    "\n",
    "### *Overview*\n",
    "Tourism is a thriving industry in Kenya, and travelers often face the challenge of choosing the right destinations for their trips. Our project aims to address this problem by creating a recommendation system that assists users in discovering personalized tourist destinations in the country.\n",
    "\n",
    "### *Problem Statement*\n",
    "\n",
    "Travelers often struggle to choose the most suitable tourist destinations for their trips. With an overwhelming number of options available, personalized recommendations are crucial. Our project aims to address this challenge by creating a recommendation system that suggests relevant destinations in Kenya based on user preferences and historical interactions.\n",
    "\n",
    "#### *Stakeholders*\n",
    "1. **Travelers**: They seek relevant recommendations based on their preferences, interests, and historical interactions.\n",
    "2. **Tourism Agencies**: These organizations can enhance user experiences by providing tailored suggestions.\n",
    "3. **Local Businesses**: Recommendations can drive footfall to local attractions, restaurants, and accommodations.\n",
    "\n",
    "### *Objectives*:\n",
    "\n",
    "- \"Build a collaborative filtering model to recommend destinations.\"\n",
    "- \"Reduce cold-start problem by incorporating content-based features.\"\n",
    "- \"Model Recall score ≥ 80%\"\n",
    "- \"Model Accuracy ≥ 80%\"\n",
    "\n",
    "### *Proposed Solution and Metrics of Success*\n",
    "We propose building a hybrid recommendation system that combines collaborative filtering and content-based approaches. Success metrics include accuracy, recall and precision scores.\n",
    "\n",
    "### *Challenges*\n",
    "\n",
    "1. **Data Quality and Diversity**:\n",
    "   - Presence of missing values, outliers, or inaccuracies.\n",
    "   - Ensuring diverse and representative data across different types of destinations (e.g., cities, beaches, historical sites) is essential.\n",
    "\n",
    "2. **Cold-Start Problem**:\n",
    "   - New users with limited interaction history pose a challenge. How do we recommend destinations for them?\n",
    "   - Balancing collaborative filtering (based on user behavior) with content-based filtering (based on destination features) is critical.\n",
    "\n",
    "3. **Scalability and Real-Time Recommendations**:\n",
    "   - As the user base grows, the system must handle increased computational demands.\n",
    "   - Providing real-time recommendations during user interactions requires efficient algorithms.\n",
    "\n",
    "4. **User Engagement and Interpretability**:\n",
    "   - Recommendations should align with user interests to keep them engaged.\n",
    "   - Ensuring transparency and interpretability of the recommendation process is important.\n",
    "\n",
    "### *Conclusion*\n",
    "Our project has significant implications for travelers, tourism agencies, and local businesses. By solving this problem, we contribute to enhancing travel experiences and promoting local economies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Understanding***\n",
    "\n",
    "### *Data Sources and Relevance*\n",
    "- The dataset was scraped using the **APIFY Tripadvisor Scraper**.\n",
    "- It contains information about tourist destinations, including their names, categories, ratings, review counts, images, and other relevant features.\n",
    "- The data's relevance lies in its ability to help us recommend destinations to travelers based on their preferences and historical interactions.\n",
    "\n",
    "### *Dataset Overview*\n",
    "- The dataset consists of **2567 entries** (rows).\n",
    "- Key columns include:\n",
    "  - **Name**: The name of the destination.\n",
    "  - **Category**: The type of destination (e.g., city, beach, historical site).\n",
    "  - **Rating**: The average user rating (ranging from 1.0 to 5.0).\n",
    "  - **Number of Reviews**: The count of user reviews.\n",
    "  - **Image**: URLs to images representing the destinations.\n",
    "  - **Photo Count**: The number of photos associated with each destination.\n",
    "  - **Price Range**: Information about the cost level (if available).\n",
    "  - **Review Tags**: Descriptive tags associated with reviews.\n",
    "  - **Photos**: Additional photo URLs.\n",
    "  - **Price Level**: Indication of price range (if available).\n",
    "\n",
    "### *Justification for Feature Inclusion*\n",
    "- **Name**, **Category**, and **Rating**: Essential for personalized recommendations.\n",
    "- **Number of Reviews**: Reflects popularity and user engagement.\n",
    "- **Image** and **Photo Count**: Enhance user experience.\n",
    "- **Price Range** and **Price Level**: Useful for budget-conscious travelers.\n",
    "- **Review Tags**: Provides insights into user preferences.\n",
    "\n",
    "### *Data Limitations*\n",
    "- **Missing Values**: Some entries lack ratings, images, or price information.\n",
    "- **Limited Price Data**: Only 1487 entries have price-related details.\n",
    "- **Data Quality**: Ensure data quality and handle missing values appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Preparation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import ast\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import itertools\n",
    "\n",
    "\n",
    "# !pip install textblob\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD, NMF, KNNWithMeans, SVDpp\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from surprise.prediction_algorithms.matrix_factorization import NMF\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVDpp\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ndcg_score, average_precision_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD, NMF, KNNWithMeans, SVDpp\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy as sup_accuracy\n",
    "from surprise.prediction_algorithms.matrix_factorization import NMF\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVDpp\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "# Ignore future deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "\n",
    "class DataCleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_json_files(self, json_files, expected_columns):\n",
    "    # Reads multiple JSON files and concatenates them into a single DataFrame\n",
    "        dfs = []\n",
    "        for file in json_files:\n",
    "            with open(file, encoding='utf-8', errors='ignore') as f:\n",
    "                json_data = json.load(f)\n",
    "                df = pd.DataFrame([\n",
    "                        {\n",
    "                            col: item.get(col, np.nan) for col in expected_columns\n",
    "                        }\n",
    "                        for item in json_data                 \n",
    "                \n",
    "                    \n",
    "                ])\n",
    "                dfs.append(df)\n",
    "                self.df = pd.concat(dfs, ignore_index=True)\n",
    "                \n",
    "        return self.df\n",
    "    \n",
    "   \n",
    "    def drop_columns(self, columns, df):\n",
    "        # Drops specified columns from the DataFrame\n",
    "        self.df.drop(columns=columns, inplace=True)\n",
    "        \n",
    "    def missing_values_percentage(self, df):\n",
    "        # Calculates the percentage of missing values in each column\n",
    "        column_percentages = self.df.isnull().sum() / len(self.df) * 100\n",
    "        columns_with_missing_values = column_percentages[column_percentages > 0]\n",
    "        return columns_with_missing_values.sort_values(ascending=False) \n",
    "    \n",
    "    def drop_above_threshold(self, threshold):\n",
    "        # Drops columns with missing values percentage above the specified threshold\n",
    "        column_percentages = self.missing_values_percentage(self.df)\n",
    "        columns_with_missing_values = column_percentages[column_percentages > threshold]\n",
    "        columns_to_drop = columns_with_missing_values.index.tolist()\n",
    "        self.df.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "    def split_price_range(self):\n",
    "        # Splits the priceRange column into LowerPrice and UpperPrice columns\n",
    "        self.df[['LowerPrice', 'UpperPrice']] = self.df['priceRange'].str.replace('KES', '').str.split(' - ', expand=True)\n",
    "        self.df['LowerPrice'] = self.df['LowerPrice'].str.replace(',', '').astype(float)\n",
    "        self.df['UpperPrice'] = self.df['UpperPrice'].str.replace(',', '').astype(float)\n",
    "\n",
    "    def fill_missing_prices(self):\n",
    "        # Fills missing values in LowerPrice and UpperPrice columns based on type (ATTRACTION or HOTEL)\n",
    "        self.df.loc[self.df['type'] == 'ATTRACTION', 'LowerPrice'] = self.df.loc[self.df['type'] == 'ATTRACTION', 'LowerPrice'].fillna(self.df['LowerPrice'].min())\n",
    "        self.df.loc[self.df['type'] == 'ATTRACTION', 'UpperPrice'] = self.df.loc[self.df['type'] == 'ATTRACTION', 'UpperPrice'].fillna(self.df['UpperPrice'].min())\n",
    "        self.df.loc[self.df['type'] == 'HOTEL', 'LowerPrice'] = self.df.loc[self.df['type'] == 'HOTEL', 'LowerPrice'].fillna(self.df['LowerPrice'].mean())\n",
    "        self.df.loc[self.df['type'] == 'HOTEL', 'UpperPrice'] = self.df.loc[self.df['type'] == 'HOTEL', 'UpperPrice'].fillna(self.df['UpperPrice'].mean())\n",
    "\n",
    "\n",
    "    def clean_ratings(self):\n",
    "        # Replaces missing values in the rating column with 0\n",
    "        self.df['rating'].fillna(5, inplace=True)\n",
    "\n",
    "    def clean_review_tags(self):\n",
    "        # Cleans up the reviewTags column by extracting the text values\n",
    "        self.df.loc[:, 'reviewTags'] = self.df['reviewTags'].apply(lambda entries: [{'text': entry['text']} for entry in entries] if isinstance(entries, list) else [])\n",
    "        self.df.loc[:, 'reviewTags'] = self.df['reviewTags'].apply(lambda tags: [tag['text'] for tag in tags])\n",
    "\n",
    "\n",
    "    def drop_missing_values(self, columns):\n",
    "        # Drops rows with missing values in specified columns\n",
    "        self.df = self.df.dropna(subset=columns)\n",
    "\n",
    "    def drop_unused_columns(self, columns):\n",
    "        # Drops unused columns from the DataFrame\n",
    "        self.df = self.df.drop(columns=columns)\n",
    "\n",
    "    def drop_rows_with_nan(self):\n",
    "        # Drops rows with NaN values\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "    def save_to_csv(self, file_path):\n",
    "        # Saves the DataFrame to a CSV file\n",
    "        self.df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Data Cleaning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataSourcing:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "\n",
    "    def read_json_files(self, json_files, expected_columns):\n",
    "    # Reads multiple JSON files and concatenates them into a single DataFrame\n",
    "        dfs = []\n",
    "        for file in json_files:\n",
    "            with open(file, encoding='utf-8', errors='ignore') as f:\n",
    "                json_data = json.load(f)\n",
    "                df = pd.DataFrame([\n",
    "                        {\n",
    "                            col: item.get(col, np.nan) for col in expected_columns\n",
    "                        }\n",
    "                        for item in json_data                 \n",
    "                \n",
    "                    \n",
    "                ])\n",
    "                dfs.append(df)\n",
    "                self.df = pd.concat(dfs, ignore_index=True)\n",
    "                \n",
    "        return self.df\n",
    "    \n",
    "    def dataframe_details(self,df):\n",
    "        \"\"\"\n",
    "        Print details of the dataframe.\n",
    "        Parameters:\n",
    "        df (DataFrame): The dataframe to be analyzed.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        print(\"============================\")\n",
    "        print(f\"DATAFRAME SHAPE: {df.shape}\")\n",
    "        print(\"============================\\n\\n\")\n",
    "        print(\"================\")\n",
    "        print(f\"DATAFRAME HEAD:\")\n",
    "        print(\"================\")\n",
    "        print(f\"{df.head()}\")\n",
    "        print(\"========================================================================\\n\\n\")\n",
    "        print(\"=======================\")\n",
    "        print(f\"DATAFRAME COLUMNS INFO:\")\n",
    "        print(\"=======================\")\n",
    "        print(f\"{df.info()}\")\n",
    "        print(\"========================================================================\\n\\n\")\n",
    "        print(\"==========================\")\n",
    "        print(f\"DATAFRAME KEY STATISTICS:\")\n",
    "        print(\"==========================\")\n",
    "        print(f\"{df.describe().transpose()}\")\n",
    "        print(\"========================================================================\\n\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DataFrame info***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataSourcing()\n",
    "\n",
    "files = [\n",
    "        \"data\\kenya.json\", \"data\\Tripadvisor1.json\",\n",
    "        \"data\\Tripadvisor2.json\", \"data\\Tripadvisor3.json\", \"data\\Tripadvisor4.json\"\n",
    "        ]\n",
    "expected_columns = [\n",
    "        \"name\", \"category\", \"rating\", \"numberOfReviews\",\n",
    "        \"image\", \"photoCount\", \"priceRange\", \"reviewTags\",\n",
    "        \"priceLevel\", \"locationString\"\n",
    "                    ]\n",
    "\n",
    "data = ds.read_json_files(files, expected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dataframe_details(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DataCleaning class\n",
    "data_cleaner = DataCleaning()\n",
    "\n",
    "# Assign your existing data DataFrame to the class's df attribute\n",
    "data_cleaner.df = data\n",
    "\n",
    "# Apply the missing_values_percentage method\n",
    "missing_values = data_cleaner.missing_values_percentage(data_cleaner.df)\n",
    "\n",
    "# Print the results\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Check for Duplicates***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list columns to strings to avoid unhashable type errors\n",
    "data['reviewTags'] = data['reviewTags'].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "# data['photos'] = data['photos'].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.duplicated()\n",
    "\n",
    "# Display duplicates\n",
    "print(\"Duplicate Rows:\")\n",
    "duplicates.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Drop Duplicates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate rows\n",
    "data = data.drop_duplicates()\n",
    "data.drop_duplicates(subset=['name'], inplace=True)\n",
    "# Display the cleaned data\n",
    "print('Cleaned Data Shape:')\n",
    "print(data.shape)\n",
    "print('='*45)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Feature Engineering***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing(DataSourcing):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"Data Preprocessing class that inherits from the data sourcing class.\n",
    "        Contains functions to be used to check certain aspects in the data for cleaning.\n",
    "        Checks for duplicates, nulls and outliers\n",
    "        \"\"\"\n",
    "      \n",
    "\n",
    "    def check_duplicates(self, data):\n",
    "        duplicates = data[data.duplicated()].shape[0]\n",
    "        print(\"There are {} duplicates in the data.\".format(duplicates))\n",
    "        \n",
    "    def check_null_values(self, data):\n",
    "        null_values = data.isnull().sum()\n",
    "        print(null_values)\n",
    "        print(\"====================================\")\n",
    "        print(\"List of columns with missing values:\")\n",
    "        print(\"====================================\")\n",
    "        return null_values[null_values > 0].index.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data preprocessing class\n",
    "dp = DataPreprocessing()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Extract `reviews` and `texts` from `reviewTags`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string representations of lists of dictionaries to actual lists of dictionaries\n",
    "data['reviewTags'] = data['reviewTags'].apply(ast.literal_eval)\n",
    "\n",
    "# Extract texts and reviews from the reviewTags column\n",
    "data['texts'] = data['reviewTags'].apply(lambda x: [d['text'] for d in x])\n",
    "data['reviews'] = data['reviewTags'].apply(lambda x: [d['reviews'] for d in x])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Decode the `priceLevel` column***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['priceLevel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Map the `$` into their corresponding categories*\n",
    "\n",
    "1. **Luxury**: The most expensive category, offering premium services and facilities.\n",
    "\n",
    "2. **Premium**: Mid-range in price, providing high-quality services and accommodations.\n",
    "\n",
    "3. **Standard**: Affordable options with good services and facilities.\n",
    "\n",
    "4. **Budget**: The most economical choice, offering basic services and accommodations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function\n",
    "def map_dollar_signs(dollar_signs):\n",
    "    mapping = {\n",
    "        \"$$$$\": \"Luxury\",\n",
    "        \"$$$\": \"Premium\",\n",
    "        \"$$\": \"Standard\",\n",
    "        \"$\": \"Budget\"\n",
    "    }\n",
    "    return mapping.get(dollar_signs, \"Unknown\")\n",
    "\n",
    "# Create new column 'priceLevel'\n",
    "data['priceLevel'] = data['priceLevel'].apply(map_dollar_signs)\n",
    "\n",
    "# Display the DataFrame\n",
    "data['priceLevel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Create new price columns (`upperPrice` & `lowerPrice`)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_price_range(data, conversion_rate=145.0):\n",
    "    # Extract unique values from the priceRange column\n",
    "    unique_price_ranges = data['priceRange'].nunique()\n",
    "\n",
    "    # Print the number of unique values (optional)\n",
    "    print(f\"Number of unique price ranges: {unique_price_ranges}\")\n",
    "\n",
    "    # Convert USD prices to KES and extract lowerPrice and upperPrice\n",
    "    def convert_and_extract(x):\n",
    "        if isinstance(x, str):\n",
    "            if '$' in x:\n",
    "                lower_price = float(x.split(' - ')[0].replace('$', '').replace(',', '')) * conversion_rate\n",
    "                upper_price = float(x.split(' - ')[-1].replace('$', '').replace(',', '')) * conversion_rate\n",
    "            elif 'KES' in x:\n",
    "                lower_price = float(x.split(' - ')[0].replace('KES', '').replace(',', ''))\n",
    "                upper_price = float(x.split(' - ')[-1].replace('KES', '').replace(',', ''))\n",
    "            return lower_price, upper_price\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    data[['lowerPrice', 'upperPrice']] = data['priceRange'].apply(\n",
    "        lambda x: pd.Series(convert_and_extract(x))\n",
    "    )\n",
    "\n",
    "    # Fill missing values with NaN\n",
    "    data['lowerPrice'] = data['lowerPrice'].fillna(np.nan)\n",
    "    data['upperPrice'] = data['upperPrice'].fillna(np.nan)\n",
    "\n",
    "    return data[['lowerPrice', 'upperPrice']].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_price_range(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['upperPrice', 'lowerPrice']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['upperPrice', 'lowerPrice']].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4. Create new column (`averagePrice`)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['averagePrice'] = (data['lowerPrice'] + data['upperPrice']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['averagePrice'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5. Create new column (`weighted_sentiment`) for Sentiment Analysis of Reviews***\n",
    "\n",
    "##### *STEPS*\n",
    "\n",
    "    Step 1: Perform Sentiment Analysis\n",
    "\n",
    "    - We will use TextBlob to calculate the sentiment polarity. (Polarity is a float within the range [-1.0, 1.0], where -1.0 represents a negative sentiment and 1.0 represents a positive sentiment.)\n",
    "\n",
    "    Step 2: Weight the Sentiment Scores\n",
    "\n",
    "    Step 3: Aggregate the Weighted Scores\n",
    "\n",
    "  \n",
    "\n",
    "We will create an additional column, `weighted_sentiment`, that represents the weighted average sentiment score for each place. This column can be used to make recommendations based on the overall sentiment of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis and calculate weighted sentiment scores\n",
    "def calculate_weighted_sentiment(texts, reviews):\n",
    "    sentiments = [TextBlob(text).sentiment.polarity for text in texts]\n",
    "    weighted_sentiments = [sentiment * review for sentiment, review in zip(sentiments, reviews)]\n",
    "    total_reviews = sum(reviews)\n",
    "    weighted_average_sentiment = sum(weighted_sentiments) / total_reviews if total_reviews > 0 else 0\n",
    "    return weighted_average_sentiment\n",
    "\n",
    "# The statistical method below helps in balancing the sentiment score with the number of reviews, preventing\n",
    "# entities with few reviews from ranking disproportionately high or low.\n",
    "def bayesian_average(sentiment, num_reviews, C=10, m=0.03):\n",
    "    \"\"\"\n",
    "    sentiment: Weighted sentiment score\n",
    "    num_reviews: Number of reviews\n",
    "    C: Weight parameter (the number of reviews we deem sufficient for confidence)\n",
    "    m: Prior mean sentiment (average sentiment across all entities)\n",
    "    \"\"\"\n",
    "    return (C * m + sentiment * num_reviews) / (C + num_reviews)\n",
    "\n",
    "data['weighted_sentiment'] = data.apply(lambda row: calculate_weighted_sentiment(row['texts'], row['reviews']), axis=1)\n",
    "\n",
    "data['adjusted_sentiment'] = data.apply(\n",
    "    lambda row: bayesian_average(\n",
    "        row['weighted_sentiment'], \n",
    "        row['numberOfReviews'], \n",
    "        C=10, \n",
    "        m=data['weighted_sentiment'].mean()\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "review_data = data[['name','texts','reviews', 'numberOfReviews', 'weighted_sentiment', 'adjusted_sentiment']]\n",
    "\n",
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reviews'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5. Extract the `location` & `province` from `locationString`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings or null-like values with np.nan\n",
    "data['locationString'] = data['locationString'].apply(lambda x: np.nan if pd.isnull(x) or x.strip() == '' else x)\n",
    "\n",
    "# Fill remaining null values with a placeholder (e.g., \"Unknown, Unknown\")\n",
    "# data['locationString'] = data['locationString'].fillna('Unknown, Unknown')\n",
    "\n",
    "# Extract the location (before the first comma)\n",
    "data['location'] = data['locationString'].str.split(',', n=1).str[0].str.strip()\n",
    "\n",
    "# Extract the province (after the last comma)\n",
    "data['province'] = data['locationString'].str.split(',').str[-1].str.strip()\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(data[['location', 'province']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6. Create a new value `tour operator` in the `category column`***\n",
    "\n",
    "There are tour operators categorized as attractions. Let's separate them into their own category for better classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keywords to search for\n",
    "# keywords = ['safari', 'safaris', 'tour', 'tours', 'adventure', 'adventures']\n",
    "\n",
    "# # Function to update category only if name contains keywords\n",
    "# def update_category(row):\n",
    "#     name, current_category = row\n",
    "#     if any(keyword in name.lower() for keyword in keywords):\n",
    "#         return 'tour operator'\n",
    "#     return current_category\n",
    "\n",
    "# # Apply the function to update the 'category' column\n",
    "# data['category'] = data[['name', 'category']].apply(update_category, axis=1)\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['safari', 'safaris', 'tour', 'tours', 'adventure',\n",
    "            'adventures', 'expeditions', 'expedition', 'travels',\n",
    "            'travel', 'travellers', 'escursioni']\n",
    "hotel_keywords = ['cottages', 'spa', 'lodge', 'camp', 'club', 'hotel', 'resort']\n",
    "\n",
    "# Function to update category with priority for hotel-related words\n",
    "def update_category(row):\n",
    "    name, current_category = row\n",
    "    name_lower = name.lower()\n",
    "\n",
    "    # Check for hotel-related keywords first\n",
    "    if any(hotel_keyword in name_lower for hotel_keyword in hotel_keywords):\n",
    "        return 'hotel'\n",
    "    \n",
    "    # Check for other keywords\n",
    "    if any(keyword in name_lower for keyword in keywords):\n",
    "        return 'tour operator'\n",
    "    \n",
    "    return current_category\n",
    "\n",
    "# Apply the function to update the 'category' column\n",
    "data['category'] = data[['name', 'category']].apply(update_category, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***EDA***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Visual of the Destinations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image URLs\n",
    "# Load the first 9 images\n",
    "image_urls = data['image'].tolist()\n",
    "images = []\n",
    "\n",
    "# Download images from URLs\n",
    "for url in image_urls[0:9]:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(io.BytesIO(response.content))\n",
    "            images.append(img)\n",
    "        else:\n",
    "            images.append(None)  # Append None if the image couldn't be retrieved\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching image from {url}: {e}\")\n",
    "        images.append(None)\n",
    "\n",
    "# Create a 3x3 grid of images\n",
    "num_rows, num_cols = 3, 3\n",
    "image_matrix = [[None for _ in range(num_cols)] for _ in range(num_rows)]\n",
    "\n",
    "for idx, img in enumerate(images):\n",
    "    row, col = divmod(idx, num_cols)\n",
    "    if row < num_rows:\n",
    "        image_matrix[row][col] = img\n",
    "\n",
    "# Plot the images with labels\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        if image_matrix[i][j] is not None:\n",
    "            axs[i, j].imshow(image_matrix[i][j])\n",
    "            axs[i, j].set_title(data.loc[i * num_cols + j, 'name'])\n",
    "        axs[i, j].axis('off')  # Hide axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Review Bigrams Visual***\n",
    "\n",
    "A visualization of the most common bigrams in the reviews, weighted by the number of reviews, can be useful for the tour recommendation system to find the most popular words used in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts into bigrams and count frequencies\n",
    "def get_bigrams(texts, reviews):\n",
    "    bigram_counts = Counter()\n",
    "    for text, review in zip(texts, reviews):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        bigrams_list = list(bigrams(tokens))\n",
    "        for bigram in bigrams_list:\n",
    "            bigram_counts[bigram] += review\n",
    "    return bigram_counts\n",
    "\n",
    "data['bigram_counts'] = data.apply(lambda row: get_bigrams(row['texts'], row['reviews']), axis=1)\n",
    "\n",
    "# Flatten the bigrams into a single string for vectorization\n",
    "def flatten_bigrams(bigram_counts):\n",
    "    flattened_text = []\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        flattened_text.extend([' '.join(bigram)] * count)\n",
    "    return ' '.join(flattened_text)\n",
    "\n",
    "data['flattened_bigrams'] = data['bigram_counts'].apply(flatten_bigrams)\n",
    "\n",
    "# Aggregate bigram counts across all rows\n",
    "total_bigram_counts = Counter()\n",
    "for counts in data['bigram_counts']:\n",
    "    total_bigram_counts.update(counts)\n",
    "\n",
    "# Get the most common bigrams\n",
    "most_common_bigrams = total_bigram_counts.most_common(10)\n",
    "bigram_labels, bigram_values = zip(*most_common_bigrams)\n",
    "\n",
    "# Convert tuples to strings for labels\n",
    "bigram_labels = [' '.join(bigram) for bigram in bigram_labels]\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_data = pd.DataFrame({'Bigram': bigram_labels, 'Count': bigram_values})\n",
    "\n",
    "# Visualize the word counts\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Count', y='Bigram', data=plot_data, palette='magma')\n",
    "plt.xlabel('Weighted Review Count')\n",
    "plt.ylabel('Bigrams')\n",
    "plt.title('Top 10 Bigrams by Weighted Review Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extract the main bigram that describes the destinations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the bigram with the highest frequency\n",
    "def extract_main_bigram(counter):\n",
    "    if not counter:  # Check if the Counter is empty\n",
    "        return None\n",
    "    return max(counter, key=counter.get)\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['main_bigram'] = data['bigram_counts'].apply(extract_main_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the bigram counts further\n",
    "top_n = 50  # Further reduce the number of bigrams\n",
    "limited_bigram_frequencies = dict(total_bigram_counts.most_common(top_n))\n",
    "print(f\"Number of bigrams: {len(limited_bigram_frequencies)}\")\n",
    "print(f\"Memory usage: {sum([len(str(k)) + len(str(v)) for k, v in limited_bigram_frequencies.items()]) / (1024 * 1024):.2f} MB\")\n",
    "import gc\n",
    "\n",
    "# Clear variables\n",
    "del total_bigram_counts\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all texts into a single string\n",
    "combined_texts = ' '.join(' '.join(texts) for texts in data['texts'])\n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='magma').generate(combined_texts)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Frequent Words in Reviews')\n",
    "plt.show()\n",
    "# Print the most common words\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    # Remove non-alphabetic characters and words with less than 3 characters\n",
    "    words = [word for word in words if word.isalpha() and len(word) > 2]\n",
    "    return words\n",
    "\n",
    "# Preprocess and count words\n",
    "words = preprocess(all_reviews)\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Print the 20 most common words\n",
    "print(\"20 most common words:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Key Themes\n",
    "\n",
    "- **Safari and Wildlife:** The dominant theme is wildlife experiences, highlighted by terms like \"safari,\" \"wildlife,\" \"animals,\" \"game drive,\" and specific animals such as lion, elephant, and giraffe. This suggests a strong emphasis on safari and wildlife activities.\n",
    "\n",
    "- **Accommodation and Service:** Key terms include \"lodge,\" \"camp,\" \"hotel,\" \"service,\" and \"staff,\" reflecting a focus on guest accommodation and overall service quality.\n",
    "\n",
    "- **Location:** Geographic terms such as \"Masai Mara,\" \"Lake Naivasha,\" \"Kenya,\" and \"Nairobi\" underscore the regional focus of the reviews.\n",
    "\n",
    "- **Positive Sentiment:** Words like \"amazing,\" \"great,\" \"beautiful,\" and \"wonderful\" point to an overall positive sentiment towards the reviewed experiences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewtags by rating\n",
    "high_rated = data[data['rating'] >= 4.5]\n",
    "\n",
    "def extract_tags(tags_list):\n",
    "    return [tag['text'] for tag in tags_list]\n",
    "\n",
    "all_tags = [tag for tags in high_rated['reviewTags'] for tag in extract_tags(tags)]\n",
    "top_tags = Counter(all_tags).most_common(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(y=[tag[0] for tag in top_tags], x=[tag[1] for tag in top_tags], palette='viridis')\n",
    "plt.title('Top 10 Review Tags for Highly-Rated Attractions (by Rating)')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Tags')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the top tags and their counts\n",
    "print(\"Top 10 tags for highly-rated attractions:\")\n",
    "for tag, count in top_tags:\n",
    "    print(f\"{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewtags by review count\n",
    "high_rated = data[data['rating'] >= 4.5]\n",
    "\n",
    "def extract_tags_with_reviews(tags_list):\n",
    "    return [(tag['text'], tag['reviews']) for tag in tags_list]\n",
    "\n",
    "all_tags_with_reviews = [item for tags in high_rated['reviewTags'] for item in extract_tags_with_reviews(tags)]\n",
    "tag_review_counts = Counter()\n",
    "for tag, reviews in all_tags_with_reviews:\n",
    "    tag_review_counts[tag] += reviews\n",
    "\n",
    "top_tags_by_reviews = tag_review_counts.most_common(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(y=[tag[0] for tag in top_tags_by_reviews], x=[tag[1] for tag in top_tags_by_reviews], palette='viridis')\n",
    "plt.title('Top 10 Review Tags for Highly-Rated Attractions (by Review Count)')\n",
    "plt.xlabel('Total Review Count')\n",
    "plt.ylabel('Tags')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 tags for highly-rated attractions by total review count:\")\n",
    "for tag, count in top_tags_by_reviews:\n",
    "    print(f\"{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in data['category'].unique():\n",
    "    # Join all texts for the current category into a single string\n",
    "    category_texts = ' '.join(' '.join(texts) for texts, cat in zip(data['texts'], data['category']) if cat == category)\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(category_texts)\n",
    "    \n",
    "    # Plot word cloud\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for Category: {category}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Insights\n",
    "\n",
    "#### Category: \"Attraction\"\n",
    "\n",
    "**Dominant Words:** Safari, Kenya, Guide, Park, Driver, Animal, Experience, Great, National Park, Company\n",
    "\n",
    "**Key Insights:**\n",
    "- Safari is the primary attraction, with Kenya being a strong associated term.\n",
    "- Guided experiences and park visits are crucial aspects.\n",
    "- The presence of \"driver\" and \"company\" suggests organized tours are popular.\n",
    "- Positive sentiment is indicated by words like \"experience\" and \"great.\"\n",
    "\n",
    "#### Category: \"Hotel\"\n",
    "\n",
    "**Dominant Words:** Camp, Game, Stay, Food, Animal, Lodge, Pool, Room Service, Hot Water, Tent\n",
    "\n",
    "**Key Insights:**\n",
    "- This category focuses on accommodation and facilities within a safari setting.\n",
    "- Camping and lodges are popular options.\n",
    "- Amenities like food, pool, and hot water are valued.\n",
    "- The word \"animal\" suggests proximity to wildlife is important.\n",
    "\n",
    "#### Category: \"Tour Operator\"\n",
    "\n",
    "**Dominant Words:** Company, Safari, Guide, Team, Knowledge, Masai Mara, Itinerary, Communication, Driver, Animal\n",
    "\n",
    "**Key Insights:**\n",
    "- This category emphasizes the role of the operator in providing a safari experience.\n",
    "- Expertise and knowledge are important attributes of the operator.\n",
    "- The presence of \"Masai Mara\" and \"itinerary\" suggests specific destinations and planned trips.\n",
    "- Good communication and a strong team are valued.\n",
    "\n",
    "### Overall Insights:\n",
    "- Safari in Kenya is the core attraction.\n",
    "- Guided tours and organized experiences are preferred.\n",
    "- Accommodation options range from camping to lodges with amenities.\n",
    "- Operators play a crucial role in providing a successful safari experience.\n",
    "- Positive sentiment towards the overall experience is evident.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(texts, place_name, ax):\n",
    "    combined_texts = ' '.join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='magma').generate(combined_texts)\n",
    "    \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Word Cloud for {place_name}')\n",
    "\n",
    "# Number of rows and columns for the subplot grid\n",
    "n_rows = 2\n",
    "n_cols = 3\n",
    "\n",
    "# Create a subplot grid\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 10))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate and plot word clouds for the top 5 places\n",
    "for i in range(min(5, len(axes))):\n",
    "    place_name = data.iloc[i]['name']\n",
    "    texts = data.iloc[i]['texts']\n",
    "    plot_wordcloud(texts, place_name, axes[i])\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(axes)):\n",
    "    if j >= 5:\n",
    "        axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Insights from the places:\n",
    "\n",
    "- **Diversity of Experiences:** The word clouds reveal a diverse range of experiences offered by these locations. From adventurous hikes to historical explorations and serene farm visits, there's something for everyone.\n",
    "- **Emphasis on Nature and Culture:** Nature and cultural heritage are recurring themes across the word clouds, suggesting that visitors appreciate these aspects.\n",
    "- **Positive Sentiment:** Overall, the word clouds convey a positive sentiment about these places. Visitors use words like \"amazing,\" \"beautiful,\" and \"interesting,\" indicating enjoyable experiences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Univariate Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data, exclude_columns=[]):\n",
    "    for col in data.columns:\n",
    "        # Skip the column if it's in the exclude_columns list\n",
    "        if col in exclude_columns:\n",
    "            continue\n",
    "        \n",
    "        if data[col].dtype == 'object' or data[col].nunique() < 20:\n",
    "            # If the column is categorical or has less than 20 unique values, use countplot\n",
    "            custom_palette = 'magma'\n",
    "            sns.countplot(data=data, x=col, palette=custom_palette)\n",
    "            plt.title(f\"Distribution of '{col}'\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks()\n",
    "            plt.show()\n",
    "        elif pd.api.types.is_numeric_dtype(data[col]):\n",
    "            # Plot histogram with KDE for numerical columns\n",
    "            sns.histplot(data[col], kde=True, stat=\"density\")\n",
    "            plt.title(f\"Distribution of '{col}'\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Density')\n",
    "            plt.show()\n",
    "\n",
    "# Example usage\n",
    "exclude = ['name', 'image', 'priceRange', 'reviewTags', 'photos', 'reviews', 'texts', 'adjusted_sentiment',\n",
    "           'bigram_counts', 'main_bigram', 'location','locationString','province', 'flattened_bigrams']\n",
    "plot_distribution(data, exclude_columns=exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers in rating, number of reviews, and price range\n",
    "data['averagePrice'] = (data['lowerPrice'] + data['upperPrice']) / 2\n",
    "\n",
    "# Create a figure with a 2x2 grid of subplots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Violin plot for rating distribution\n",
    "sns.violinplot(x=data['rating'], ax=ax1, inner='quartile', color='skyblue')\n",
    "ax1.set_title('Rating Distribution', fontsize=14)\n",
    "ax1.set_xlabel('Rating', fontsize=12)\n",
    "\n",
    "# Violin plot for number of reviews distribution\n",
    "sns.violinplot(x=data['numberOfReviews'], ax=ax2, inner='quartile', color='lightgreen')\n",
    "ax2.set_title('Number of Reviews Distribution', fontsize=14)\n",
    "ax2.set_xlabel('Number of Reviews', fontsize=12)\n",
    "\n",
    "# Violin plot for average price distribution\n",
    "sns.violinplot(x=data['averagePrice'], ax=ax3, inner='quartile', color='lightcoral')\n",
    "ax3.set_title('Average Price Distribution', fontsize=14)\n",
    "ax3.set_xlabel('Average Price', fontsize=12)\n",
    "\n",
    "# Hide the fourth subplot (bottom-right) as we only have three plots\n",
    "ax4.axis('off')\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "data[['rating', 'numberOfReviews', 'averagePrice']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Insights\n",
    "- **Rating Distribution:** The distribution is heavily skewed towards higher ratings, with a sharp peak around 4.5 and a long tail towards lower ratings. This suggests that while most reviews are positive, there's still a significant portion with lower ratings.\n",
    "  \n",
    "- **Number of Reviews Distribution:** The distribution is skewed, with most businesses having a lower number of reviews and a few outliers with a large number of reviews, indicating a few businesses dominate in visibility.\n",
    "  \n",
    "- **Average Price Distribution:** The distribution is highly skewed, with the majority of businesses having an average price below 1.0. A few outliers have significantly higher average prices.\n",
    "\n",
    "#### Insights\n",
    "- **Customer Satisfaction:** The high concentration of ratings around 4.5 suggests overall customer satisfaction is relatively high, though there's room for improvement as indicated by the long tail of lower ratings.\n",
    "  \n",
    "- **Market Competition:** The skewed distribution of the number of reviews suggests a competitive market, with a few businesses attracting most of the customer attention.\n",
    "  \n",
    "- **Price Sensitivity:** The highly skewed average price distribution indicates that the market largely caters to budget-conscious customers, suggesting that businesses with higher prices may need to focus on differentiation beyond just price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Correlation Analysis***\n",
    "\n",
    "Label encode the priceLevel so as to perform a correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['priceLevelencoded'] = label_encoder.fit_transform(data['priceLevel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df,column=None,rank=None):\n",
    "    '''\n",
    "    performs correlation matrix, drops non numeric object types\n",
    "    variables are dataframe, column, and ranking\n",
    "    ranking is optional\n",
    "    column = 'column'\n",
    "    rank = True for # rank 1,2,3...\n",
    "    \n",
    "    '''\n",
    "    # Redundant check. Makes sure all columns of object type are dropped\n",
    "    df_object = df.select_dtypes(include='object')\n",
    "    print(f'The following columns were dropped due to being object types:\\n{df_object.columns.tolist()}')\n",
    "    df = df.drop(df_object,axis=1) \n",
    "    df1 = df\n",
    "    if rank is True:\n",
    "        try:\n",
    "            df_corr = df1.corr()[column]\n",
    "            df_corr = df_corr.drop(column)\n",
    "            print(f'Ranked correlation to {column}')\n",
    "            return(df_corr.rank(ascending=False).sort_values())\n",
    "        except:\n",
    "            df_corr = df1.corr()\n",
    "            return(df_corr.rank(ascending=False))\n",
    "    else:    \n",
    "        try:\n",
    "            df_corr = df.corr()[column]\n",
    "            df_corr = df_corr.drop(column)\n",
    "            print(f'Correlation to {column}')\n",
    "            return(df_corr.sort_values(ascending=False))\n",
    "        except:\n",
    "            df_corr = df1.corr()\n",
    "            return(df_corr)\n",
    "\n",
    "# Getting the correlation between variables\n",
    "df_corr = abs(correlation(data))\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(df_corr[['priceLevelencoded']].sort_values(by='priceLevelencoded',ascending=False),annot = True)\n",
    "\n",
    "ax.set_title('Variables Correlating with Price Level');\n",
    "# plt.savefig('Group_plots/correlation_plot.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Correlation with Price Level:\n",
    "\n",
    "- **Perfect Correlation:** `priceLevelEncoded` has a perfect correlation with itself (1.0), as expected.\n",
    "- **Rating:** Shows a strong positive correlation with price level (0.31), indicating that higher-priced listings tend to have higher ratings.\n",
    "- **Lower Price:** Has a weak positive correlation (0.056) with price level, which might seem counterintuitive and warrants further investigation.\n",
    "- **Photo Count:** Exhibits a moderate positive correlation (0.033) with price level, suggesting that listings with more photos tend to be priced higher.\n",
    "- **Number of Reviews:** Displays a weak positive correlation (0.017) with price level, indicating a slight tendency for listings with more reviews to have higher prices.\n",
    "- **Sentiment:** Both `weighted_sentiment` and `adjusted_sentiment` have very weak positive correlations (0.015 and 0.013, respectively) with price level, suggesting minimal impact of sentiment on price.\n",
    "- **Upper Price:** Shows a very weak positive correlation (0.0088) with price level, indicating a negligible relationship between upper price and overall price level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "numerical_columns = [\n",
    "    'rating',\n",
    "    'numberOfReviews',\n",
    "    'photoCount',\n",
    "    'lowerPrice',\n",
    "    'upperPrice',\n",
    "    'weighted_sentiment',\n",
    "    'adjusted_sentiment',\n",
    "    'averagePrice'\n",
    "]\n",
    "\n",
    "corr_matrix = data[numerical_columns].corr()\n",
    "# Create a fig size\n",
    "plt.figure(figsize=(16, 16))\n",
    "# Create a mask to show only the lower triangle\n",
    "mask = np.zeros_like(corr_matrix, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Plot the heatmap with the lower triangle mask applied\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='magma', center=0, annot=True)\n",
    "plt.title('Correlation Matrix of Numerical Features', fontsize=20)\n",
    "# Show the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Insights:\n",
    "\n",
    "- **`Photo Count` and `Number of Reviews`:** Have a moderate positive correlation (0.46), suggesting that restaurants with more reviews tend to have more photos.\n",
    "- **`Rating` and `Adjusted Sentiment`:** Show a moderate positive correlation (0.37), indicating that higher ratings tend to be associated with more positive sentiment.\n",
    "- **`Review Count` and `Rating`:** Display a moderate positive correlation (0.27), suggesting that restaurants with more reviews tend to have higher ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Bivariate Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specified size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# First subplot: Weighted Sentiment vs Rating\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.regplot(x='weighted_sentiment', y='rating', data=data, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "plt.title('Weighted Sentiment vs Rating', fontsize=14)\n",
    "plt.xlabel('Weighted Sentiment', fontsize=12)\n",
    "plt.ylabel('Rating', fontsize=12)\n",
    "\n",
    "# Second subplot: Adjusted Sentiment vs Rating\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.regplot(x='adjusted_sentiment', y='rating', data=data, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "plt.title('Adjusted Sentiment vs Rating', fontsize=14)\n",
    "plt.xlabel('Adjusted Sentiment', fontsize=12)\n",
    "plt.ylabel('Rating', fontsize=12)\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is no discernible linear relationship or pattern suggesting that higher sentiment values consistently correspond to higher ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Relationship between price range and rating\n",
    "data['averagePrice'] = (data['lowerPrice'] + data['upperPrice']) / 2\n",
    "\n",
    "# Create a figure with a specified size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with regression line\n",
    "sns.regplot(x='averagePrice', y='rating', data=data, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Average Price vs Rating', fontsize=16)\n",
    "plt.xlabel('Average Price', fontsize=14)\n",
    "plt.ylabel('Rating', fontsize=14)\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limited Correlation:** There appears to be a weak positive correlation between average price and rating. This means that, generally, as the average price increases, the rating tends to increase slightly as well. However, this relationship is not very strong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Correlation between number of reviews and rating\n",
    "correlation = data['numberOfReviews'].corr(data['rating'])\n",
    "\n",
    "# Create a figure with a specified size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with regression line\n",
    "sns.regplot(x='numberOfReviews', y='rating', data=data, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(f'Number of Reviews vs Rating (Correlation: {correlation:.2f})', fontsize=16)\n",
    "plt.xlabel('Number of Reviews', fontsize=14)\n",
    "plt.ylabel('Rating', fontsize=14)\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  A weak negative correlation between the number of reviews and the rating. This means that as the number of reviews increases, the rating tends to slightly decrease, but the relationship is not very strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Correlation between number of photos and rating/reviews\n",
    "photo_rating_corr = data['photoCount'].corr(data['rating'])\n",
    "photo_reviews_corr = data['photoCount'].corr(data['numberOfReviews'])\n",
    "\n",
    "print(f\"Correlation between photo count and rating: {photo_rating_corr:.2f}\")\n",
    "print(f\"Correlation between photo count and number of reviews: {photo_reviews_corr:.2f}\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot with regression line for photo count vs rating\n",
    "sns.regplot(x='photoCount', y='rating', data=data, ax=axes[0], scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "axes[0].set_title(f'Photo Count vs Rating\\nCorrelation: {photo_rating_corr:.2f}')\n",
    "axes[0].set_xlabel('Photo Count')\n",
    "axes[0].set_ylabel('Rating')\n",
    "\n",
    "# Scatter plot with regression line for photo count vs number of reviews\n",
    "sns.regplot(x='photoCount', y='numberOfReviews', data=data, ax=axes[1], scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "axes[1].set_title(f'Photo Count vs Number of Reviews\\nCorrelation: {photo_reviews_corr:.2f}')\n",
    "axes[1].set_xlabel('Photo Count')\n",
    "axes[1].set_ylabel('Number of Reviews')\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " **Photo Count and Rating:** The quality of the photos, rather than the quantity, likely has a greater impact on the rating.\n",
    "Focus on uploading high-quality, relevant images to showcase the listing effectively.\n",
    "\n",
    "\n",
    "\n",
    "**Photo Count and Reviews:** More photos can attract attention and encourage users to engage with the listing, leading to more reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Sentiment vs Adjusted Sentiment \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x='weighted_sentiment', y='adjusted_sentiment', data=data, scatter_kws={'s':10}, line_kws={'color':'red'})\n",
    "plt.title('Weighted Sentiment vs Adjusted Sentiment')\n",
    "plt.xlabel('Weighted Sentiment')\n",
    "plt.ylabel('Adjusted Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strong Relationship:** strong positive relationship,This indicates that the adjustment process applied to the original sentiment scores preserves the overall sentiment directionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for Province Distribution\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.countplot(y='province', data=data, order=data['province'].value_counts().index, palette='coolwarm')\n",
    "plt.title('Distribution of Provinces', fontsize=16)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Province')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Aggregate data by province\n",
    "province_aggregates = data.groupby('province').agg({\n",
    "    'rating': 'mean',\n",
    "    'numberOfReviews': 'sum',\n",
    "    'weighted_sentiment': 'mean',\n",
    "    'adjusted_sentiment': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "province_aggregates.columns = ['Province', 'Average Rating', 'Total Reviews', 'Average Weighted Sentiment', 'Average Adjusted Sentiment']\n",
    "\n",
    "# Display the aggregated data\n",
    "province_aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'province_aggregates' is your DataFrame\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 14))\n",
    "\n",
    "# Bar plot for Average Rating by Province\n",
    "sns.barplot(y='Province', x='Average Rating', data=province_aggregates, palette='coolwarm', ax=ax1)\n",
    "ax1.set_title('Average Rating by Province', fontsize=16)\n",
    "ax1.set_xlabel('Average Rating', fontsize=14)\n",
    "ax1.set_ylabel('Province', fontsize=14)\n",
    "\n",
    "# Bar plot for Total Reviews by Province\n",
    "sns.barplot(y='Province', x='Total Reviews', data=province_aggregates, palette='viridis', ax=ax2)\n",
    "ax2.set_title('Total Reviews by Province', fontsize=16)\n",
    "ax2.set_xlabel('Total Reviews', fontsize=14)\n",
    "ax2.set_ylabel('Province', fontsize=14)\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for Average Sentiment by Province\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.scatterplot(y='Province', x='Average Weighted Sentiment', data=province_aggregates, size='Total Reviews', sizes=(50, 500), hue='Average Rating', palette='coolwarm', legend=False)\n",
    "plt.title('Average Sentiment by Province', fontsize=16)\n",
    "plt.xlabel('Average Weighted Sentiment')\n",
    "plt.ylabel('Province')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **High Sentiment:** A few provinces stand out with notably higher sentiment scores:\n",
    "  - Nairobi\n",
    "  - Rift Valley Province\n",
    "  - Coast Province\n",
    "  \n",
    "  This could indicate factors contributing to positive sentiment\n",
    "- **Low Sentiment:** Several provinces exhibit lower sentiment scores, with some clustering near the 0.00 mark:\n",
    "  - Central Province\n",
    "  - Western Province\n",
    "  - North Eastern Province\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_aggregates = data.groupby('province').agg({\n",
    "    'rating': 'mean',\n",
    "    'numberOfReviews': 'sum',\n",
    "    'weighted_sentiment': 'mean',\n",
    "    'adjusted_sentiment': 'mean'\n",
    "}).reset_index()\n",
    "#print(province_aggregates)\n",
    "\n",
    "top_locations = data.groupby(['province', 'name']).agg({\n",
    "    'rating': 'mean',\n",
    "    'numberOfReviews': 'sum',\n",
    "    'weighted_sentiment': 'mean'\n",
    "}).sort_values(by='rating', ascending=False).reset_index()\n",
    "top_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Top 10  Locations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude rows where location or province is None\n",
    "filtered_data = data.dropna(subset=['location', 'province'])\n",
    "\n",
    "# Re-calculate the top 10 names based on average rating after excluding rows with missing values\n",
    "top_10_names = filtered_data.groupby('name').agg({\n",
    "    'rating': 'mean',\n",
    "    'category': 'first',\n",
    "    'location': 'first',\n",
    "    'province': 'first'\n",
    "}).nlargest(10, 'rating')\n",
    "\n",
    "# Reset index to get 'name' as a column\n",
    "top_10_names = top_10_names.reset_index()\n",
    "\n",
    "# Display the table with name, rating, category, location, and province\n",
    "print(\"Top 10 Names by Average Rating:\")\n",
    "top_10_names[['name', 'rating', 'category', 'location', 'province']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean rating and total number of reviews by category\n",
    "category_analysis = data.groupby('category').agg({\n",
    "    'rating': 'mean',\n",
    "    'numberOfReviews': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort the data\n",
    "category_analysis_sorted_by_rating = category_analysis.sort_values(by='rating', ascending=False)\n",
    "category_analysis_sorted_by_reviews = category_analysis.sort_values(by='numberOfReviews', ascending=False)\n",
    "\n",
    "# Set the figure size and style \n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for mean rating by category\n",
    "plt.subplot(2, 1, 1)  # 2 rows, 1 column, 1st subplot\n",
    "sns.barplot(x='rating', y='category', data=category_analysis_sorted_by_rating, palette='viridis')\n",
    "plt.title('Mean Rating by Category')\n",
    "plt.xlabel('Mean Rating')\n",
    "plt.ylabel('Category')\n",
    "\n",
    "# Plot for total number of reviews by category\n",
    "plt.subplot(2, 1, 2)  # 2 rows, 1 column, 2nd subplot\n",
    "sns.barplot(x='numberOfReviews', y='category', data=category_analysis_sorted_by_reviews, palette='magma')\n",
    "plt.title('Total Number of Reviews by Category')\n",
    "plt.xlabel('Total Number of Reviews')\n",
    "plt.ylabel('Category')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Rating and Number of Reviews by Category\n",
    "review_stats_by_category = data.groupby('category')[['rating', 'numberOfReviews']].mean().reset_index()\n",
    "review_stats_by_category.columns = ['Category', 'Average Rating', 'Average Number of Reviews']\n",
    "review_stats_by_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tour Operators Shine\n",
    "- Tour operators have the highest average rating, indicating strong customer satisfaction with their services.\n",
    "- However, they have the lowest number of reviews, which could suggest less exposure or a smaller tourists base compared to other categories.\n",
    "\n",
    "##### Hotels Dominate Reviews\n",
    "- Hotels receive the most reviews, making them the most reviewed category.\n",
    "- This could be due to a larger number of hotels or a higher tendency for tourists to leave reviews for hotel experiences.\n",
    "\n",
    "##### Attraction Satisfaction\n",
    "- Attractions fall in the middle for both average rating and total number of reviews.\n",
    "- This suggests a balance between tourists satisfaction and review volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Distribution of ratings across different categories\n",
    "g = sns.FacetGrid(data, col=\"category\", col_wrap=4, height=4)\n",
    "g.map(sns.histplot, \"rating\", discrete=True)\n",
    "\n",
    "# Set the titles and labels\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Rating\", \"Count\")\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hotel Ratings\n",
    "\n",
    "- The **hotel** category exhibits the highest overall count of ratings.\n",
    "- It displays a pronounced peak around the rating of **5**, suggesting a larger concentration of positive reviews for hotels compared to the other two categories.\n",
    "\n",
    "#### Attraction and Tour Operator: Room for Improvement\n",
    "\n",
    "- Both **attractions** and **tour operators** might have room for improvement in customer satisfaction, as evidenced by their lower rating counts and flatter distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data, col=\"priceLevel\", col_wrap=4, height=4)\n",
    "g.map(sns.countplot, \"category\", order=data['category'].value_counts().index)\n",
    "\n",
    "# Set the titles and labels\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Category\", \"Count\")\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category Dominance\n",
    "\n",
    "- **Hotels** are the most prevalent category across all price levels, with a particularly high count in the \"**Budget**\" segment.\n",
    "- This suggests a strong preference for budget-friendly hotels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table = data.groupby(['category', 'priceLevel']).agg(\n",
    "    avg_rating=('rating', 'mean'),\n",
    "    total_reviews=('numberOfReviews', 'sum'),\n",
    "    avg_photo_count=('photoCount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "combined_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Average price range for different categories\n",
    "data['averagePrice'] = (data['lowerPrice'] + data['upperPrice']) / 2\n",
    "avg_price_by_category = data.groupby('category')['averagePrice'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_price_by_category.plot(kind='bar')\n",
    "plt.title('Average Price by Category')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The attraction category lacks price data and this presents a challenge. We will handle the issue by creating separate models for each category.\n",
    "\n",
    "We will build three different recommendation models: one for attractions, tour oprerator and the other for hotels. Since attractions don’t have price data, we will exclude priceLevel from the model for attractions and focus on other features like rating and reviews.\n",
    "\n",
    "This approach will allow tailoring the recommendation model specifically to the available data for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('C:/Users/Hp/Documents/DATA_SCIENCE/MORINGA/PHASE_5-Final_Project/SafariHub/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Modeling***\n",
    "\n",
    "Encode the `location`, `province` and `category` columns so as to fit them into the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['location_encoded'] = label_encoder.fit_transform(data['location'])\n",
    "data['province_encoded'] = label_encoder.fit_transform(data['province'])\n",
    "data['category_encoded'] = label_encoder.fit_transform(data['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into categories so as to work with the different requirements.\n",
    "This is to avoid dropping rows that don't have price values which would be helpful for the hotel category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where the 'category' is 'attraction'\n",
    "attraction_data = data[data['category'] == 'attraction'][[\n",
    "    'name', 'category', 'rating', 'numberOfReviews', 'photoCount','category_encoded', 'location',\n",
    "    'reviewTags', 'priceLevel', 'texts', 'reviews', 'weighted_sentiment', 'adjusted_sentiment',\n",
    "    'bigram_counts', 'priceLevelencoded', 'location_encoded', 'province_encoded', 'flattened_bigrams'\n",
    "]]\n",
    "\n",
    "# Filter the rows where the 'category' is 'hotel'\n",
    "hotel_data = data[data['category'] == 'hotel'][[\n",
    "    'name', 'category', 'rating', 'numberOfReviews', 'photoCount','category_encoded',\n",
    "    'priceRange', 'reviewTags', 'priceLevel', 'texts', 'reviews',\n",
    "    'lowerPrice', 'upperPrice', 'weighted_sentiment', 'adjusted_sentiment',\n",
    "    'bigram_counts', 'priceLevelencoded', 'location_encoded', 'province_encoded', 'flattened_bigrams'\n",
    "]]\n",
    "\n",
    "# Filter the rows where the 'category' is 'tours'\n",
    "tours_data = data[data['category'] == 'tour operator'][[\n",
    "    'name', 'category', 'rating', 'numberOfReviews', 'photoCount','category_encoded', 'main_bigram',\n",
    "    'reviewTags', 'priceLevel', 'texts', 'reviews', 'weighted_sentiment', 'adjusted_sentiment',\n",
    "    'bigram_counts', 'priceLevelencoded', 'location_encoded', 'province_encoded', 'flattened_bigrams'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attraction_data.shape)\n",
    "print(hotel_data.shape)\n",
    "print(tours_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attraction_data = attraction_data.dropna()\n",
    "attraction_data.reset_index(drop=True, inplace=True)\n",
    "print(attraction_data.shape)\n",
    "hotel_data = hotel_data.dropna()\n",
    "hotel_data.reset_index(drop=True, inplace=True)\n",
    "print(hotel_data.shape)\n",
    "tours_data = tours_data.dropna()\n",
    "tours_data.reset_index(drop=True, inplace=True)\n",
    "print(tours_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attraction predictor\n",
    "\n",
    "### Model 1 - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity condition to be used for classification\n",
    "attraction_data['similar'] = ((attraction_data['rating'].diff().abs() < 0.5) & \n",
    "                              (attraction_data['priceLevelencoded'].diff().abs() == 0))\n",
    "\n",
    "# Convert the condition to binary labels (1 for similar, 0 for not similar)\n",
    "attraction_data['similar'] = attraction_data['similar'].astype(int)\n",
    "\n",
    "names = attraction_data['name']\n",
    "\n",
    "# Assign the target variable 'y'\n",
    "y = attraction_data['similar']\n",
    "\n",
    "# Add 'similar' as a feature to X\n",
    "X = attraction_data[['category_encoded', 'rating', 'numberOfReviews', 'photoCount', 'adjusted_sentiment',\n",
    "                     'location_encoded', 'province_encoded', 'priceLevelencoded', 'similar']]\n",
    "\n",
    "# Vectorization\n",
    "# Initialize CountVectorizer to vectorize the flattened bigrams\n",
    "vectorizer = CountVectorizer()\n",
    "bigram_matrix = vectorizer.fit_transform(attraction_data['flattened_bigrams'])\n",
    "\n",
    "# Combine this with the bigram matrix as before\n",
    "combined_features = np.hstack((X, bigram_matrix.toarray()))\n",
    "\n",
    "# Scale the combined features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, names_train, names_test = train_test_split(X_scaled, y, names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the KNN model on training data\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn.fit(X_train)\n",
    "\n",
    "def recommend_attractions(tour_name, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend attractions similar to a given tour name.\n",
    "    \n",
    "    Parameters:\n",
    "    - tour_name: The name of the tour selected by the user.\n",
    "    - top_n: The number of similar attractions to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_data: DataFrame containing recommended attraction names, ratings, price range, location, and distances.\n",
    "    \"\"\"\n",
    "    # Find the index of the given tour name\n",
    "    tour_idx = names_train[names_train == tour_name].index[0]\n",
    "    \n",
    "    # Find the nearest neighbors in the training set\n",
    "    distances, indices = knn.kneighbors([X_train[tour_idx]], n_neighbors=top_n+1)  # +1 to include the query itself\n",
    "    \n",
    "    # Get the recommended attractions, excluding the query itself\n",
    "    recommended_names = names_train.iloc[indices.flatten()[1:]].values\n",
    "    \n",
    "    # Retrieve the relevant data from attraction_data\n",
    "    recommended_data = attraction_data[attraction_data['name'].isin(recommended_names)][['name', 'rating', 'location']]\n",
    "    \n",
    "    # Add the distances to the DataFrame\n",
    "    recommended_data['distances'] = distances.flatten()[1:]\n",
    "    \n",
    "    return recommended_data\n",
    "\n",
    "# Test the Recommendation Function\n",
    "example_tour_name = names_train.iloc[23]\n",
    "recommended_attractions = recommend_attractions(example_tour_name)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_recommendations(knn_model, X_test, y_test, names_test, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the KNN model by predicting the most similar attractions for the test set\n",
    "    and calculating metrics like RMSE, MAE, Accuracy, Precision, Recall, F1 Score, and Classification Report.\n",
    "    \"\"\"\n",
    "    total_mse, total_mae, count = 0, 0, 0\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for idx, test_sample in enumerate(X_test):\n",
    "        # Get the true similarity label for this test sample\n",
    "        true_label = y_test.iloc[idx]\n",
    "        \n",
    "        # Predict the closest neighbors for the test sample\n",
    "        distances, indices = knn_model.kneighbors([test_sample], n_neighbors=top_n)\n",
    "        \n",
    "        # Check if the true name is in the recommended names\n",
    "        recommended_names = names_train.iloc[indices.flatten()].values\n",
    "        pred_labels = [1 if y_train.iloc[indices.flatten()[i]] == 1 else 0 for i in range(top_n)]\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        all_true_labels.append(true_label)\n",
    "        all_pred_labels.append(pred_labels[0])  # Take the top recommendation as the prediction\n",
    "        \n",
    "        # Calculate distance metrics (using the distance as an approximation of similarity)\n",
    "        mse = np.mean(distances**2)\n",
    "        mae = np.mean(distances)\n",
    "        \n",
    "        total_mse += mse\n",
    "        total_mae += mae\n",
    "        count += 1\n",
    "    \n",
    "    avg_mse = total_mse / count\n",
    "    avg_rmse = np.sqrt(avg_mse)\n",
    "    avg_mae = total_mae / count\n",
    "    \n",
    "    print(f\"Distance Metrics:\\n\")\n",
    "    print(f\"Average Mean Squared Error (MSE): {avg_mse}\\n\")\n",
    "    print(f\"Average Root Mean Squared Error (RMSE): {avg_rmse}\\n\")\n",
    "    print(f\"Average Mean Absolute Error (MAE): {avg_mae}\")\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    # Generate and print the classification report\n",
    "    report = classification_report(all_true_labels, all_pred_labels, target_names=[\"Not Similar\", \"Similar\"])\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_recommendations(knn, X_test, y_test, names_test, top_n=5)\n",
    "\n",
    "\n",
    "print(f\"Recommended attractions for {example_tour_name}:\")\n",
    "recommended_attractions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MSE` of `0.610` indicates that the squared differences between predicted and actual ratings are relatively small. The `RMSE` of `0.781` suggests that, on average, our predictions are about 0.781 units away from the actual ratings. `MAE` of `0.711` shows that, on average, our predictions deviate by 0.711 units from the true ratings, indicating overall good model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine Similarity\n",
    "similarity_matrix = cosine_similarity(X_scaled)\n",
    "\n",
    "# Function to recommend attractions based on cosine similarity\n",
    "def recommend_attractions(attraction_index, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend attractions based on cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - attraction_index: The index of the hotel for which recommendations are being generated.\n",
    "    - top_n: The number of similar attractions to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_data: DataFrame containing recommended attraction names, ratings, locations, and similarity scores.\n",
    "    - similarity_scores: List of similarity scores corresponding to the recommended attractions.\n",
    "    \"\"\"\n",
    "    similarity_scores = list(enumerate(similarity_matrix[attraction_index]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    similarity_scores = similarity_scores[1:top_n+1]  # Exclude the item itself\n",
    "    attraction_indices = [i[0] for i in similarity_scores]\n",
    "    \n",
    "    # Extract the recommended attractions from the original data\n",
    "    recommended_data = attraction_data.iloc[attraction_indices].copy()\n",
    "    \n",
    "    # Add the similarity scores to the recommended data\n",
    "    recommended_data['similarity_score'] = [score[1] for score in similarity_scores]\n",
    "    \n",
    "    return recommended_data, [score[1] for score in similarity_scores]\n",
    "\n",
    "# Function to evaluate the cosine similarity-based recommendation model\n",
    "def evaluate_cosine_recommendations(similarity_matrix, attraction_data, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the cosine similarity-based recommendation model by predicting the most similar attractions\n",
    "    and calculating metrics like RMSE, MAE, Accuracy, Precision, Recall, F1 Score, and Classification Report.\n",
    "    \"\"\"\n",
    "    total_mse, total_mae, count = 0, 0, 0\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for idx in range(len(similarity_matrix)):\n",
    "        # Get the true label (1 if the hotel is similar, 0 if not)\n",
    "        true_label = attraction_data['similar'].iloc[idx]\n",
    "        \n",
    "        # Get the top N recommended attractions for this hotel\n",
    "        recommended_attractions, distances = recommend_attractions(idx, top_n=top_n)\n",
    "        \n",
    "        # Predict label based on whether the recommended hotel is similar\n",
    "        pred_labels = [1 if hotel['similar'] == 1 else 0 for _, hotel in recommended_attractions.iterrows()]\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        all_true_labels.append(true_label)\n",
    "        all_pred_labels.append(pred_labels[0])  # Take the top recommendation as the prediction\n",
    "        \n",
    "        # Calculate distance metrics (using the distance as an approximation of similarity)\n",
    "        mse = np.mean(np.array(distances)**2)\n",
    "        mae = np.mean(np.array(distances))\n",
    "        \n",
    "        total_mse += mse\n",
    "        total_mae += mae\n",
    "        count += 1\n",
    "    \n",
    "    avg_mse = total_mse / count\n",
    "    avg_rmse = np.sqrt(avg_mse)\n",
    "    avg_mae = total_mae / count\n",
    "    \n",
    "    print(f\"Distance Metrics:\\n\")\n",
    "    print(f\"Average Mean Squared Error (MSE): {avg_mse}\\n\")\n",
    "    print(f\"Average Root Mean Squared Error (RMSE): {avg_rmse}\\n\")\n",
    "    print(f\"Average Mean Absolute Error (MAE): {avg_mae}\")\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # print(f\"\\nClassification Metrics:\\n\")\n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    # Generate and print the classification report\n",
    "    report = classification_report(all_true_labels, all_pred_labels, target_names=[\"Not Similar\", \"Similar\"])\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Evaluate the cosine similarity-based recommendations\n",
    "evaluate_cosine_recommendations(similarity_matrix, attraction_data, top_n=5)\n",
    "\n",
    "# Get recommendations for a specific attraction\n",
    "attraction_index = 0  # Example index\n",
    "attraction_name = attraction_data['name'].iloc[attraction_index]\n",
    "recommended_attractions, similarity_scores = recommend_attractions(attraction_index, top_n=5)\n",
    "\n",
    "print(f\"Recommended attractions for {attraction_name}:\")\n",
    "recommended_attractions[['name', 'rating', 'location', 'similarity_score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity condition\n",
    "# Hotels are similar if they have a rating difference of less than 0.5 and are within the same price level\n",
    "attraction_data['similar'] = ((attraction_data['rating'].diff().abs() < 0.5) & \n",
    "                         (attraction_data['priceLevelencoded'].diff().abs() == 0))\n",
    "\n",
    "# Convert the condition to binary labels (1 for similar, 0 for not similar)\n",
    "attraction_data['similar'] = attraction_data['similar'].astype(int)\n",
    "\n",
    "# Assign the target variable 'y'\n",
    "y = attraction_data['similar']\n",
    "\n",
    "# Select the features and the target labels\n",
    "X = attraction_data[['category_encoded', 'rating', 'numberOfReviews', 'photoCount', 'adjusted_sentiment',\n",
    "           'location_encoded', 'province_encoded', 'priceLevelencoded']]\n",
    "\n",
    "# Extract the 'name' column\n",
    "names = attraction_data['name']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the SVM model on training data\n",
    "svm_model = SVC(kernel='linear')  # You can use other kernels like 'rbf' if needed\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "def recommend_attractions(tour_name, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend attractions similar to a given tour name based on SVM classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - tour_name: The name of the tour selected by the user.\n",
    "    - top_n: The number of similar attractions to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_attractions: DataFrame of recommended attraction names, ratings, and locations.\n",
    "    \"\"\"\n",
    "    # Find the index of the given tour name\n",
    "    attraction_idx = names[names == tour_name].index[0]\n",
    "    \n",
    "    # Predict similarities for all tours\n",
    "    predictions = svm_model.decision_function(X_scaled)\n",
    "    \n",
    "    # Sort the indices by the decision function values (the larger, the more similar)\n",
    "    similar_indices = np.argsort(predictions)[::-1]\n",
    "    \n",
    "    # Get the recommended attractions, excluding the query itself\n",
    "    recommended_attractions = attraction_data.iloc[similar_indices[1:top_n+1]][['name', 'rating', 'location']]\n",
    "    return recommended_attractions\n",
    "\n",
    "\n",
    "# Evaluate the SVM model\n",
    "def evaluate_svm(svm_model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the SVM model by predicting the labels for the test set\n",
    "    and printing classification metrics.\n",
    "    \"\"\"\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_svm(svm_model, X_test, y_test)\n",
    "\n",
    "# Test the Recommendation Function\n",
    "example_name = names.iloc[20]  # Just taking the first attraction as an example\n",
    "recommended_attractions = recommend_attractions(example_name)\n",
    "print(f\"\\nAttractions similar to {example_name}:\")\n",
    "recommended_attractions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observation***\n",
    "\n",
    "- Both KNN & Cosine Similarity models have their strengths and weaknesses. The KNN model seems to perform better in terms of precision, while the cosine similarity model has a lower RMSE.\n",
    "- SVM performed worse in all aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hotel predictor\n",
    "\n",
    "### Model 1 - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity condition to be used for classification\n",
    "hotel_data['similar'] = ((hotel_data['rating'].diff().abs() < 0.5) & \n",
    "                              (hotel_data['priceLevelencoded'].diff().abs() == 0))\n",
    "\n",
    "# Convert the condition to binary labels (1 for similar, 0 for not similar)\n",
    "hotel_data['similar'] = hotel_data['similar'].astype(int)\n",
    "\n",
    "names = hotel_data['name']\n",
    "\n",
    "# Assign the target variable 'y'\n",
    "y = hotel_data['similar']\n",
    "\n",
    "# Add 'similar' as a feature to X\n",
    "X = hotel_data[['category_encoded', 'rating', 'numberOfReviews', 'photoCount', 'adjusted_sentiment',\n",
    "                     'location_encoded', 'province_encoded', 'priceLevelencoded', 'similar',\n",
    "                     'upperPrice', 'lowerPrice']]\n",
    "\n",
    "# Vectorization\n",
    "# Initialize CountVectorizer to vectorize the flattened bigrams\n",
    "vectorizer = CountVectorizer()\n",
    "bigram_matrix = vectorizer.fit_transform(hotel_data['flattened_bigrams'])\n",
    "\n",
    "# Combine this with the bigram matrix as before\n",
    "combined_features = np.hstack((X, bigram_matrix.toarray()))\n",
    "\n",
    "# Scale the combined features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, names_train, names_test = train_test_split(X_scaled, y, names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the KNN model on training data\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn.fit(X_train)\n",
    "\n",
    "def recommend_hotels(hotel_name, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend hotels similar to a given hotel name.\n",
    "    \n",
    "    Parameters:\n",
    "    - hotel_name: The name of the hotel selected by the user.\n",
    "    - top_n: The number of similar hotels to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_data: DataFrame containing recommended hotel names, ratings, price range, location, and distances.\n",
    "    \"\"\"\n",
    "    # Find the index of the given hotel name\n",
    "    hotel_idx = names_train[names_train == hotel_name].index[0]\n",
    "    \n",
    "    # Find the nearest neighbors in the training set\n",
    "    distances, indices = knn.kneighbors([X_train[hotel_idx]], n_neighbors=top_n+1)  # +1 to include the query itself\n",
    "    \n",
    "    # Get the recommended hotels, excluding the query itself\n",
    "    recommended_names = names_train.iloc[indices.flatten()[1:]].values\n",
    "    \n",
    "    # Retrieve the relevant data from hotel_data\n",
    "    recommended_data = hotel_data[hotel_data['name'].isin(recommended_names)][['name', 'rating', 'priceRange', 'priceLevel']]\n",
    "    \n",
    "    # Add the distances to the DataFrame\n",
    "    recommended_data['distances'] = distances.flatten()[1:]\n",
    "    \n",
    "    return recommended_data\n",
    "\n",
    "# Test the Recommendation Function\n",
    "example_hotel_name = names_train.iloc[87]\n",
    "recommended_hotels = recommend_hotels(example_hotel_name)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_recommendations(knn_model, X_test, y_test, names_test, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the KNN model by predicting the most similar hotels for the test set\n",
    "    and calculating metrics like RMSE, MAE, Accuracy, Precision, Recall, F1 Score, and Classification Report.\n",
    "    \"\"\"\n",
    "    total_mse, total_mae, count = 0, 0, 0\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for idx, test_sample in enumerate(X_test):\n",
    "        # Get the true similarity label for this test sample\n",
    "        true_label = y_test.iloc[idx]\n",
    "        \n",
    "        # Predict the closest neighbors for the test sample\n",
    "        distances, indices = knn_model.kneighbors([test_sample], n_neighbors=top_n)\n",
    "        \n",
    "        # Check if the true name is in the recommended names\n",
    "        recommended_names = names_train.iloc[indices.flatten()].values\n",
    "        pred_labels = [1 if y_train.iloc[indices.flatten()[i]] == 1 else 0 for i in range(top_n)]\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        all_true_labels.append(true_label)\n",
    "        all_pred_labels.append(pred_labels[0])  # Take the top recommendation as the prediction\n",
    "        \n",
    "        # Calculate distance metrics (using the distance as an approximation of similarity)\n",
    "        mse = np.mean(distances**2)\n",
    "        mae = np.mean(distances)\n",
    "        \n",
    "        total_mse += mse\n",
    "        total_mae += mae\n",
    "        count += 1\n",
    "    \n",
    "    avg_mse = total_mse / count\n",
    "    avg_rmse = np.sqrt(avg_mse)\n",
    "    avg_mae = total_mae / count\n",
    "    \n",
    "    print(f\"Distance Metrics:\\n\")\n",
    "    print(f\"Average Mean Squared Error (MSE): {avg_mse}\\n\")\n",
    "    print(f\"Average Root Mean Squared Error (RMSE): {avg_rmse}\\n\")\n",
    "    print(f\"Average Mean Absolute Error (MAE): {avg_mae}\")\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    # Generate and print the classification report\n",
    "    report = classification_report(all_true_labels, all_pred_labels, target_names=[\"Not Similar\", \"Similar\"])\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_recommendations(knn, X_test, y_test, names_test, top_n=5)\n",
    "\n",
    "\n",
    "print(f\"Recommended hotels for {example_hotel_name}:\")\n",
    "recommended_hotels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine Similarity\n",
    "similarity_matrix = cosine_similarity(X_scaled)\n",
    "\n",
    "# Function to recommend hotels based on cosine similarity\n",
    "def recommend_hotels(hotel_index, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend hotels based on cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - hotel_index: The index of the hotel for which recommendations are being generated.\n",
    "    - top_n: The number of similar hotels to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_data: DataFrame containing recommended hotel names, ratings, locations, and similarity scores.\n",
    "    - similarity_scores: List of similarity scores corresponding to the recommended hotels.\n",
    "    \"\"\"\n",
    "    similarity_scores = list(enumerate(similarity_matrix[hotel_index]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    similarity_scores = similarity_scores[1:top_n+1]  # Exclude the item itself\n",
    "    hotel_indices = [i[0] for i in similarity_scores]\n",
    "    \n",
    "    # Extract the recommended hotels from the original data\n",
    "    recommended_data = hotel_data.iloc[hotel_indices].copy()\n",
    "    \n",
    "    # Add the similarity scores to the recommended data\n",
    "    recommended_data['similarity_score'] = [score[1] for score in similarity_scores]\n",
    "    \n",
    "    return recommended_data, [score[1] for score in similarity_scores]\n",
    "\n",
    "# Function to evaluate the cosine similarity-based recommendation model\n",
    "def evaluate_cosine_recommendations(similarity_matrix, hotel_data, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the cosine similarity-based recommendation model by predicting the most similar hotels\n",
    "    and calculating metrics like RMSE, MAE, Accuracy, Precision, Recall, F1 Score, and Classification Report.\n",
    "    \"\"\"\n",
    "    total_mse, total_mae, count = 0, 0, 0\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for idx in range(len(similarity_matrix)):\n",
    "        # Get the true label (1 if the hotel is similar, 0 if not)\n",
    "        true_label = hotel_data['similar'].iloc[idx]\n",
    "        \n",
    "        # Get the top N recommended hotels for this hotel\n",
    "        recommended_hotels, distances = recommend_hotels(idx, top_n=top_n)\n",
    "        \n",
    "        # Predict label based on whether the recommended hotel is similar\n",
    "        pred_labels = [1 if hotel['similar'] == 1 else 0 for _, hotel in recommended_hotels.iterrows()]\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        all_true_labels.append(true_label)\n",
    "        all_pred_labels.append(pred_labels[0])  # Take the top recommendation as the prediction\n",
    "        \n",
    "        # Calculate distance metrics (using the distance as an approximation of similarity)\n",
    "        mse = np.mean(np.array(distances)**2)\n",
    "        mae = np.mean(np.array(distances))\n",
    "        \n",
    "        total_mse += mse\n",
    "        total_mae += mae\n",
    "        count += 1\n",
    "    \n",
    "    avg_mse = total_mse / count\n",
    "    avg_rmse = np.sqrt(avg_mse)\n",
    "    avg_mae = total_mae / count\n",
    "    \n",
    "    print(f\"Distance Metrics:\\n\")\n",
    "    print(f\"Average Mean Squared Error (MSE): {avg_mse}\\n\")\n",
    "    print(f\"Average Root Mean Squared Error (RMSE): {avg_rmse}\\n\")\n",
    "    print(f\"Average Mean Absolute Error (MAE): {avg_mae}\")\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # print(f\"\\nClassification Metrics:\\n\")\n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    # Generate and print the classification report\n",
    "    report = classification_report(all_true_labels, all_pred_labels, target_names=[\"Not Similar\", \"Similar\"])\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Evaluate the cosine similarity-based recommendations\n",
    "evaluate_cosine_recommendations(similarity_matrix, hotel_data, top_n=5)\n",
    "\n",
    "# Get recommendations for a specific hotel\n",
    "hotel_index = 71  # Example index\n",
    "hotel_name = hotel_data['name'].iloc[hotel_index]\n",
    "recommended_hotels, similarity_scores = recommend_hotels(hotel_index, top_n=5)\n",
    "\n",
    "print(f\"Recommended hotels for {hotel_name}:\")\n",
    "recommended_hotels[['name', 'rating', 'priceRange', 'priceLevel', 'similarity_score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity condition\n",
    "# Hotels are similar if they have a rating difference of less than 0.5 and are within the same price level\n",
    "hotel_data['similar'] = ((hotel_data['rating'].diff().abs() < 0.5) & \n",
    "                         (hotel_data['priceLevelencoded'].diff().abs() == 0))\n",
    "\n",
    "# Convert the condition to binary labels (1 for similar, 0 for not similar)\n",
    "hotel_data['similar'] = hotel_data['similar'].astype(int)\n",
    "\n",
    "# Assign the target variable 'y'\n",
    "y = hotel_data['similar']\n",
    "\n",
    "# Select the features and the target labels\n",
    "X = hotel_data[['category_encoded', 'rating', 'numberOfReviews', 'photoCount', 'adjusted_sentiment',\n",
    "           'location_encoded', 'province_encoded', 'lowerPrice', 'upperPrice', 'priceLevelencoded']]\n",
    "\n",
    "# Extract the 'name' column\n",
    "names = hotel_data['name']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the SVM model on training data\n",
    "svm_model = SVC(kernel='linear')  # You can use other kernels like 'rbf' if needed\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "def recommend_hotels(tour_name, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend attractions similar to a given tour name based on SVM classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - tour_name: The name of the tour selected by the user.\n",
    "    - top_n: The number of similar attractions to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_names: List of recommended attraction names.\n",
    "    \"\"\"\n",
    "    # Find the index of the given tour name\n",
    "    tour_idx = names[names == tour_name].index[0]\n",
    "    \n",
    "    # Predict similarities for all tours\n",
    "    predictions = svm_model.decision_function(X_scaled)\n",
    "    \n",
    "    # Sort the indices by the decision function values (the larger, the more similar)\n",
    "    similar_indices = np.argsort(predictions)[::-1]\n",
    "    \n",
    "    # Get the recommended attractions, excluding the query itself\n",
    "    recommended_names = names.iloc[similar_indices[1:top_n+1]].values\n",
    "    return recommended_names\n",
    "\n",
    "# Test the Recommendation Function\n",
    "example_tour_name = names.iloc[65]  # Just taking the first tour as an example\n",
    "recommended_names = recommend_hotels(example_tour_name)\n",
    "print(\"Recommended attractions:\", recommended_names)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "def evaluate_svm(svm_model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the SVM model by predicting the labels for the test set\n",
    "    and printing classification metrics.\n",
    "    \"\"\"\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_svm(svm_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observation***\n",
    "\n",
    "- The SVM model is performing well in predicting \"Not Similar\" attractions but is failing to identify \"Similar\" ones.\n",
    "\n",
    "- The KNN model exhibits lower error metrics (MSE, RMSE, MAE) compared to the Cosine Similarity model. This suggests that the KNN model is better at minimizing errors when predicting similarity between hotels.\n",
    "\n",
    "- Both models show similar performance in terms of classification, particularly for the \"Similar\" class. However, the KNN model slightly outperforms in F1-score for the \"Similar\" class, making it more reliable in identifying similar hotels.\n",
    "\n",
    "- The recommendations provided by the KNN model seem more varied in price level and are closer in terms of distance (e.g., distances range between 0.800 and 0.898).\n",
    "\n",
    "- The recommendations by the Cosine Similarity model seem to favor hotels with slightly different price levels and generally lower similarity scores. The distance metrics are not as consistent.\n",
    "\n",
    "- The KNN model offers recommendations that are more consistent in terms of proximity to the target hotel, which is an essential factor in making relevant recommendations.\n",
    "\n",
    "> The KNN model appears to be the more suitable option for recommending hotels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tour Operator predictor\n",
    "\n",
    "### Model 1 - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity condition to be used for classification\n",
    "tours_data['similar'] = ((tours_data['rating'].diff().abs() < 0.5) & \n",
    "                              (tours_data['priceLevelencoded'].diff().abs() == 0))\n",
    "\n",
    "# Convert the condition to binary labels (1 for similar, 0 for not similar)\n",
    "tours_data['similar'] = tours_data['similar'].astype(int)\n",
    "\n",
    "names = tours_data['name']\n",
    "\n",
    "# Assign the target variable 'y'\n",
    "y = tours_data['similar']\n",
    "\n",
    "# Add 'similar' as a feature to X\n",
    "X = tours_data[['category_encoded', 'rating', 'numberOfReviews', 'photoCount', 'adjusted_sentiment',\n",
    "                     'location_encoded', 'province_encoded', 'priceLevelencoded', 'similar']]\n",
    "\n",
    "# Vectorization\n",
    "# Initialize CountVectorizer to vectorize the flattened bigrams\n",
    "vectorizer = CountVectorizer()\n",
    "bigram_matrix = vectorizer.fit_transform(tours_data['flattened_bigrams'])\n",
    "\n",
    "# Combine this with the bigram matrix as before\n",
    "combined_features = np.hstack((X, bigram_matrix.toarray()))\n",
    "\n",
    "# Scale the combined features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, names_train, names_test = train_test_split(X_scaled, y, names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the KNN model on training data\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn.fit(X_train)\n",
    "\n",
    "def recommend_tour_operators(tour_name, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend tour_operators similar to a given tour name.\n",
    "    \n",
    "    Parameters:\n",
    "    - tour_name: The name of the tour selected by the user.\n",
    "    - top_n: The number of similar tour_operators to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_data: DataFrame containing recommended tour operator names, ratings, price range, location, and distances.\n",
    "    \"\"\"\n",
    "    # Find the index of the given tour name\n",
    "    operator_idx = names_train[names_train == tour_name].index[0]\n",
    "    \n",
    "    # Find the nearest neighbors in the training set\n",
    "    distances, indices = knn.kneighbors([X_train[operator_idx]], n_neighbors=top_n+1)  # +1 to include the query itself\n",
    "    \n",
    "    # Get the recommended tour_operators, excluding the query itself\n",
    "    recommended_names = names_train.iloc[indices.flatten()[1:]].values\n",
    "    \n",
    "    # Retrieve the relevant data from tours_data\n",
    "    recommended_data = tours_data[tours_data['name'].isin(recommended_names)][['name', 'rating', 'numberOfReviews', 'main_bigram']]\n",
    "    \n",
    "    # Add the distances to the DataFrame\n",
    "    recommended_data['distances'] = distances.flatten()[1:]\n",
    "    \n",
    "    return recommended_data\n",
    "\n",
    "# Test the Recommendation Function\n",
    "example_tour_operator_name = names_train.iloc[0]\n",
    "recommended_tour_operators = recommend_tour_operators(example_tour_operator_name)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_recommendations(knn_model, X_test, y_test, names_test, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the KNN model by predicting the most similar tour_operators for the test set\n",
    "    and calculating metrics like RMSE, MAE, Accuracy, Precision, Recall, F1 Score, and Classification Report.\n",
    "    \"\"\"\n",
    "    total_mse, total_mae, count = 0, 0, 0\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for idx, test_sample in enumerate(X_test):\n",
    "        # Get the true similarity label for this test sample\n",
    "        true_label = y_test.iloc[idx]\n",
    "        \n",
    "        # Predict the closest neighbors for the test sample\n",
    "        distances, indices = knn_model.kneighbors([test_sample], n_neighbors=top_n)\n",
    "        \n",
    "        # Check if the true name is in the recommended names\n",
    "        recommended_names = names_train.iloc[indices.flatten()].values\n",
    "        pred_labels = [1 if y_train.iloc[indices.flatten()[i]] == 1 else 0 for i in range(top_n)]\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        all_true_labels.append(true_label)\n",
    "        all_pred_labels.append(pred_labels[0])  # Take the top recommendation as the prediction\n",
    "        \n",
    "        # Calculate distance metrics (using the distance as an approximation of similarity)\n",
    "        mse = np.mean(distances**2)\n",
    "        mae = np.mean(distances)\n",
    "        \n",
    "        total_mse += mse\n",
    "        total_mae += mae\n",
    "        count += 1\n",
    "    \n",
    "    avg_mse = total_mse / count\n",
    "    avg_rmse = np.sqrt(avg_mse)\n",
    "    avg_mae = total_mae / count\n",
    "    \n",
    "    print(f\"Distance Metrics:\\n\")\n",
    "    print(f\"Average Mean Squared Error (MSE): {avg_mse}\\n\")\n",
    "    print(f\"Average Root Mean Squared Error (RMSE): {avg_rmse}\\n\")\n",
    "    print(f\"Average Mean Absolute Error (MAE): {avg_mae}\")\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    # Generate and print the classification report\n",
    "    report = classification_report(all_true_labels, all_pred_labels, target_names=[\"Not Similar\", \"Similar\"])\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_recommendations(knn, X_test, y_test, names_test, top_n=5)\n",
    "\n",
    "\n",
    "print(f\"Recommended tour_operators for {example_tour_operator_name}:\")\n",
    "recommended_tour_operators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine Similarity\n",
    "similarity_matrix = cosine_similarity(X_scaled)\n",
    "\n",
    "# Function to recommend hotels based on cosine similarity\n",
    "def recommend_tour_operators(operator_index, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend hotels based on cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - operator_index: The index of the hotel for which recommendations are being generated.\n",
    "    - top_n: The number of similar hotels to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommended_data: DataFrame containing recommended hotel names, ratings, locations, and similarity scores.\n",
    "    - similarity_scores: List of similarity scores corresponding to the recommended hotels.\n",
    "    \"\"\"\n",
    "    similarity_scores = list(enumerate(similarity_matrix[operator_index]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    similarity_scores = similarity_scores[1:top_n+1]  # Exclude the item itself\n",
    "    operator_indices = [i[0] for i in similarity_scores]\n",
    "    \n",
    "    # Extract the recommended hotels from the original data\n",
    "    recommended_data = tours_data.iloc[operator_indices].copy()\n",
    "    \n",
    "    # Add the similarity scores to the recommended data\n",
    "    recommended_data['similarity_score'] = [score[1] for score in similarity_scores]\n",
    "    \n",
    "    return recommended_data, [score[1] for score in similarity_scores]\n",
    "\n",
    "# Function to evaluate the cosine similarity-based recommendation model\n",
    "def evaluate_cosine_recommendations(similarity_matrix, tours_data, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the cosine similarity-based recommendation model by predicting the most similar hotels\n",
    "    and calculating metrics like RMSE, MAE, Accuracy, Precision, Recall, F1 Score, and Classification Report.\n",
    "    \"\"\"\n",
    "    total_mse, total_mae, count = 0, 0, 0\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for idx in range(len(similarity_matrix)):\n",
    "        # Get the true label (1 if the hotel is similar, 0 if not)\n",
    "        true_label = tours_data['similar'].iloc[idx]\n",
    "        \n",
    "        # Get the top N recommended hotels for this hotel\n",
    "        recommended_tour_operators, distances = recommend_tour_operators(idx, top_n=top_n)\n",
    "        \n",
    "        # Predict label based on whether the recommended hotel is similar\n",
    "        pred_labels = [1 if hotel['similar'] == 1 else 0 for _, hotel in recommended_tour_operators.iterrows()]\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        all_true_labels.append(true_label)\n",
    "        all_pred_labels.append(pred_labels[0])  # Take the top recommendation as the prediction\n",
    "        \n",
    "        # Calculate distance metrics (using the distance as an approximation of similarity)\n",
    "        mse = np.mean(np.array(distances)**2)\n",
    "        mae = np.mean(np.array(distances))\n",
    "        \n",
    "        total_mse += mse\n",
    "        total_mae += mae\n",
    "        count += 1\n",
    "    \n",
    "    avg_mse = total_mse / count\n",
    "    avg_rmse = np.sqrt(avg_mse)\n",
    "    avg_mae = total_mae / count\n",
    "    \n",
    "    print(f\"Distance Metrics:\\n\")\n",
    "    print(f\"Average Mean Squared Error (MSE): {avg_mse}\\n\")\n",
    "    print(f\"Average Root Mean Squared Error (RMSE): {avg_rmse}\\n\")\n",
    "    print(f\"Average Mean Absolute Error (MAE): {avg_mae}\")\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # print(f\"\\nClassification Metrics:\\n\")\n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    # Generate and print the classification report\n",
    "    report = classification_report(all_true_labels, all_pred_labels, target_names=[\"Not Similar\", \"Similar\"])\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Evaluate the cosine similarity-based recommendations\n",
    "evaluate_cosine_recommendations(similarity_matrix, tours_data, top_n=5)\n",
    "\n",
    "# Get recommendations for a specific hotel\n",
    "operator_index = 33  # Example index\n",
    "tour_operator_name = tours_data['name'].iloc[operator_index]\n",
    "recommended_tour_operators, similarity_scores = recommend_tour_operators(operator_index, top_n=5)\n",
    "\n",
    "print(f\"Recommended hotels for {tour_operator_name}:\")\n",
    "recommended_tour_operators[['name', 'rating', 'numberOfReviews', 'main_bigram', 'similarity_score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observation***\n",
    "\n",
    "- The KNN model is likely the better choice for recommending \"Similar\" tour operators, despite some weaknesses in distinguishing \"Not Similar\" ones.\n",
    "\n",
    "- The Cosine Similarity model could be preferred if minimizing prediction errors (MSE, RMSE, MAE) is more critical, but its classification performance, particularly for the \"Not Similar\" class, is less robust.\n",
    "\n",
    "> Given the project's requirements, accuracy and balanced performance across both classes are more important, thus the KNN model seems more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
